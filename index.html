<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Alan Malek</div>
<div class="menu-item"><a href="index.html" class="current">home</a></div>
<div class="menu-item"><a href="papers/Alan_Malek.pdf">CV</a></div>
<div class="menu-item"><a href="minimax.html">minimax&nbsp;regret</a></div>
</td>
<td id="layout-content">
<table class="imgtable"><tr><td>
<a href="http://alanmalek.com"><img src="headshot.jpg" alt="Me" width="200x" height="301px" /></a>&nbsp;</td>
<td align="left"><p>Alan Malek</p>
<p><a href="https://lids.mit.edu/">Postdoctoral Associal at LIDS</a></p>
<p><a href="http://mit.edu/">MIT</a></p>
<p>Thesis advisor: <a href="http://www.stat.berkeley.edu/~bartlett/">Peter Bartlett</a></p>
</td></tr></table>
<h2>About me</h2>
<p>I have recently moved to MIT for a postdoc under 
<a href="http://www-stat.wharton.upenn.edu/~rakhlin/">Sasha Rakhlin</a>. Previously, I was
at UC Berkeley with <a href="http://www.stat.berkeley.edu/~bartlett/">Peter Bartlett</a>.</p>
<h2>Contact</h2>
<p><a href="mailto:amalek@mit.edu">amalek@mit.edu</a></p>
<h1>Publications</h1>
<p>Alan Malek, Yinlam Chow, Mohammad Ghavamzadeh, Sumeet Katariya. <b>Sequential Multiple Hypothesis Testing with Type I Error Control</b>, In Proceedings of AISTATS 2017.</p>
<p>Yasin Abbasi-Yadkori, Alan Malek, Peter Bartlett, Victor Gabillon. <b>Hit-and-Run for Sampling and Planning in Non-Convex Spaces</b>, In Proceedings of AISTATS 2017.</p>
<p>Wojciech Kot≈Çowski, Wouter M. Koolen, Alan Malek. <b>Online Isotonic Regression</b>,
In <i>Proceedings of The 29th Conference on Learning Theory</i>, June 2016. <img class="eq" src="eqs/11648035034-130.png" alt="[" style="vertical-align: -5px" /><a href="papers/COLT_2016_isotonic.pdf">pdf</a><img class="eq" src="eqs/11904035804-130.png" alt="]" style="vertical-align: -5px" /></p>
<p>Wouter M. Koolen, Alan Malek, Peter L. Bartlett, Yasin Abbasi-Yadkori. <b>Minimax Time Series Prediction.</b> In <i>Advances in Neural Information Processing Systems(NIPS) 28</i>, December 2015. <img class="eq" src="eqs/11648035034-130.png" alt="[" style="vertical-align: -5px" /><a href="papers/NIPS_2015.pdf">pdf</a><img class="eq" src="eqs/11904035804-130.png" alt="]" style="vertical-align: -5px" /> <img class="eq" src="eqs/11648035034-130.png" alt="[" style="vertical-align: -5px" /><a href="papers/NIPS_2015_poster.pdf">poster</a><img class="eq" src="eqs/11904035804-130.png" alt="]" style="vertical-align: -5px" /></p>
<p>Yasin Abbasi-Yadkori, Peter Bartlett, Xi Chen, Alan Malek. <b>Large-Scale Markov Decision Problems with KL Control Cost
and its Application to Crowdsourcing.</b> In <i>International Conference on Machine Learning (ICML)</i>, July 2015. <img class="eq" src="eqs/11648035034-130.png" alt="[" style="vertical-align: -5px" /><a href="papers/ICML_2015.pdf">pdf</a><img class="eq" src="eqs/11904035804-130.png" alt="]" style="vertical-align: -5px" /> <img class="eq" src="eqs/11648035034-130.png" alt="[" style="vertical-align: -5px" /><a href="papers/ICML_2015_poster.pdf">poster</a><img class="eq" src="eqs/11904035804-130.png" alt="]" style="vertical-align: -5px" /> <img class="eq" src="eqs/11648035034-130.png" alt="[" style="vertical-align: -5px" /><a href="papers/ICML_2015_slides.pdf">slides</a><img class="eq" src="eqs/11904035804-130.png" alt="]" style="vertical-align: -5px" /></p>
<p>Peter Bartlett, Wouter Koolen, Alan Malek, Eiji Takimoto and Manfred Warmuth. <b>Minimax Fixed-Design Linear Regression.</b> In <i>Proceedings of The 28th Conference on Learning Theory</i>, July 2015. <img class="eq" src="eqs/11648035034-130.png" alt="[" style="vertical-align: -5px" /><a href="papers/COLT_2015.pdf">pdf</a><img class="eq" src="eqs/11904035804-130.png" alt="]" style="vertical-align: -5px" /> <img class="eq" src="eqs/11648035034-130.png" alt="[" style="vertical-align: -5px" /><a href="papers/COLT_2015_poster.pdf">poster</a><img class="eq" src="eqs/11904035804-130.png" alt="]" style="vertical-align: -5px" /> <img class="eq" src="eqs/11648035034-130.png" alt="[" style="vertical-align: -5px" /><a href="papers/COLT_2015_slides.pdf">slides</a><img class="eq" src="eqs/11904035804-130.png" alt="]" style="vertical-align: -5px" /></p>
<p>Wouter M. Koolen, Alan Malek, Peter L. Bartlett. <b>Efficient Minimax Strategies for Square Loss Games.</b> In <i>Advances in Neural Information Processing Systems(NIPS) 27</i>, December 2014. <img class="eq" src="eqs/11648035034-130.png" alt="[" style="vertical-align: -5px" /><a href="papers/Minimax_NIPS_2014.pdf">pdf</a><img class="eq" src="eqs/11904035804-130.png" alt="]" style="vertical-align: -5px" /> <img class="eq" src="eqs/11648035034-130.png" alt="[" style="vertical-align: -5px" /><a href="papers/Minimax_NIPS_poster.pdf">poster</a><img class="eq" src="eqs/11904035804-130.png" alt="]" style="vertical-align: -5px" /></p>
<p>Alan Malek, Yasin Abbasi-Yadkori, Peter Bartlett. <b>Linear Programming for Large-Scale Markov Decision Problems</b>. In <i>International Conference on Machine Learning (ICML)</i>, July 2014. <img class="eq" src="eqs/11648035034-130.png" alt="[" style="vertical-align: -5px" /><a href="papers/MDP-LP-ICML.pdf">pdf</a><img class="eq" src="eqs/11904035804-130.png" alt="]" style="vertical-align: -5px" /> <img class="eq" src="eqs/11648035034-130.png" alt="[" style="vertical-align: -5px" /><a href="papers/MDP-LP-ICML-slides.pdf">slides</a><img class="eq" src="eqs/11904035804-130.png" alt="]" style="vertical-align: -5px" /></p>
<h2>Preprints</h2>
<p>Y. Abbasi-Yadkori, P. Bartlett, and A. Malek, <b>Linear Programming for Large-Scale Markov Decision Problems</b>, arXiv:1402.6763 [math.OC], 2014 <img class="eq" src="eqs/11648035034-130.png" alt="[" style="vertical-align: -5px" /><a href="papers/MDP-LP-arxiv.pdf">pdf.</a><img class="eq" src="eqs/11904035804-130.png" alt="]" style="vertical-align: -5px" /></p>
<h1>Talks</h1>
<p><b>Minimax Strategies for Square Loss, Linear Regression, and Time-series Prediction.</b> Machine Learning Seminar, <i>MIT</i>. August 2016. <img class="eq" src="eqs/11648035034-130.png" alt="[" style="vertical-align: -5px" /><a href="papers/MIT_2016.pdf">slides</a><img class="eq" src="eqs/11904035804-130.png" alt="]" style="vertical-align: -5px" /></p>
<p><b>Minimax Strategies for Square Loss Games.</b> Artificial Intelligence and Reinforcement Learning Seminar, <i>University of Alberta</i>. July 2016. <img class="eq" src="eqs/11648035034-130.png" alt="[" style="vertical-align: -5px" /><a href="papers/Alberta.pdf">slides</a><img class="eq" src="eqs/11904035804-130.png" alt="]" style="vertical-align: -5px" /></p>
<p><b>Planning in Large-Scale Markov Decision Problems</b>. <i>CMU</i>. June, 2016. <img class="eq" src="eqs/11648035034-130.png" alt="[" style="vertical-align: -5px" /><a href="papers/planning.pdf">slides</a><img class="eq" src="eqs/11904035804-130.png" alt="]" style="vertical-align: -5px" /></p>
<p><b>Sequential decision making: modeling how we interact with the world.</b> Keynote, Research Symposium. <i>Harker High School</i>. April 2016. <img class="eq" src="eqs/11648035034-130.png" alt="[" style="vertical-align: -5px" /><a href="papers/Harker.pdf">slides</a><img class="eq" src="eqs/11904035804-130.png" alt="]" style="vertical-align: -5px" /></p>
<p><b>Large-Scale Markov Decision Problems with KL Control Cost and its Application to Crowdsourcing.</b> <i>ICML</i>. July 2015.
<img class="eq" src="eqs/11648035034-130.png" alt="[" style="vertical-align: -5px" /><a href="papers/ICML_2015_slides.pdf">slides</a><img class="eq" src="eqs/11904035804-130.png" alt="]" style="vertical-align: -5px" /></p>
<p><b>Linear Programming for Large-Scale Markov Decision Problems</b>. <i>ICML</i>. July 2014.
<img class="eq" src="eqs/11648035034-130.png" alt="[" style="vertical-align: -5px" /><a href="papers/MDP-LP-ICML-slides.pdf">slides</a><img class="eq" src="eqs/11904035804-130.png" alt="]" style="vertical-align: -5px" /></p>
<h1>Research Interests</h1>
<p>My interests lie between optimization, machine learning, and randomized algorithms. Here is a short description of a few areas and projects:</p>
<h2>Minimax Analysis in Online Learning</h2>
<p>The major strength of most online learning algorithms (e.g. hedge, follow the perturbed leader) is that we can prove very strong guarantees on their performance. Typically, we have regret bounds that hold for any (potentially adversarial) sequence of inputs and often we have lower bounds that show that any algorithm can be made to suffer regret of the same order. However, in certain cases, we are able to tractably compute the exact minimax algorithm of certain online learning games. Here, I mean minimax in the game-theoretical sense (and not the statistics sense, which again is a statement about matching orders). Finding the minimax algorithm comes with the strongest regret guarantee: for any other algorithm, there is a data sequence that causes at least as much  regret that the minimax algorithm would suffer. Specific problems I'm interested in:</p>
<ul>
<li><p>We have the minimax algorithm for fixed-horizon squared loss. What other online settings can we compute? E.g. linear regression, online PCA, regression with Bregman loss?</p>
</li>
<li><p>The number of loss function/value function pairs with tractable minimax algorithms is very small. How far can we go in enumerating them?</p>
</li>
<li><p>Even the most famous minimax algorithm,  Normalized Maximum Likelihood, is frequently uncomputable (i.e. exponential complexity in the game length). When can we efficiently compute it?</p>
</li>
</ul>
<h2>Sequential Hypothesis Testing</h2>
<p>In classical, Neyman-Pearson statistical hypothesis testing, a fixed sample size <img class="eq" src="eqs/9984030031-130.png" alt="N" style="vertical-align: -0px" /> and decision region is calculated at the start of the test and no statistically correct conclusions can be made before the sample size is reached. This is because only the statistical deviation at <img class="eq" src="eqs/9984030031-130.png" alt="N" style="vertical-align: -0px" /> is accounted for; there is no control over the rest of the statistic's sample path. Now if we get data that is much easier than expected, there is no way to adjust the sample size without loosing control on type I and type II errors. However, such statistics and decision boundaries do exist and have been known since at least Wald's SPRT (sequential probability ratio test). </p>
<ul>
<li><p>What other sequential tests exist with type I and II error control?</p>
</li>
<li><p>How can we extend these techniques to multiple hypothesis testing?</p>
</li>
<li><p>In ML, the Multi-Arm Bandit problem (especially the best-arm variant) is well studied. What is the intersection? Can the dynamic allocation strategies be easily lifted to multiple hypothesis testing?</p>
</li>
</ul>
<h2>Very Large-state space Markov Decision Processes</h2>
<p>The planning problem for MDPs assumes full knowledge of the MDP dynamics and loss function and asks: what is the optimal policy? Traditional approaches, such as Value Iteration, scale quadratically in the number of states which becomes prohibitively expensive. If we restrict to some low dimensional family of policies, how well can we do? What conditions do we need to obtain algorithms that scale with the dimension of this family but not with the original state-space?</p>
<ul>
<li><p>Efficient policy optimization algorithms that can complete with the best policy in a low-dimensional parametric class.</p>
</li>
<li><p>Extending previous results to have stronger guarantees on special problems, e.g. MDPs with reversible policies</p>
</li>
</ul>
<h2>Applications of random Walks to learning and optimization</h2>
<p>There is a celebrated thread of papers in Theoretical Computer Science about the volume of a convex body problem. Though exact volume computations are exponential in dimension, over the last two outcomes, increasingly efficient polynomial time algorithms have been developed which all rely on the ability to generate uniform samples with some random walk over convex sets. As a consequence, we can now use random walks to sample efficiently from more general distributions e.g. log-concave distributions, and as a further consequence, optimize some non-convex functions via simulated annealing. </p>
<ul>
<li><p>How can we conduct simulated annealing on non-convex spaces?</p>
</li>
<li><p>What implications do the results on sampling from time changing distributions have on online optimization or online algorithms?</p>
</li>
<li><p>How can we Co-opt extensive log-concave mixing time theory (e.g. Langevin dynamics) for regret bounds in online convex optimization?</p>
</li>
</ul>
<h1>Teaching</h1>
<ul>
<li><p>TA for <a href="https://bcourses.berkeley.edu/courses/1409209">CS 281B/Stat 241B: Statistical Learning Theory II</a> with Prof. Bartlett, Spring 2016</p>
</li>
<li><p>TA for <a href="https://bcourses.berkeley.edu/courses/1365701">CS 281a/Stat 241a: Statistical Learning Theory I</a> with Prof. Bartlett, Fall 2015</p>
</li>
<li><p>TA for <a href="http://www.stat.berkeley.edu/~bartlett/courses/2014spring-cs281bstat241b">CS 281B/Stat 241B: Statistical Learning Theory II</a> with Prof. Bartlett, Spring 2014</p>
</li>
<li><p>TA for <a href="http://ptolemy.eecs.berkeley.edu/eecs20/">EECS 20N: Signals and Systems</a> with Prof. Abbeel and Prof. Lee, Spring 2011</p>
</li>
<li><p>TA for <a href="http://ptolemy.eecs.berkeley.edu/eecs20/">EECS 20N: Signals and Systems</a> with Prof. Ayazifar and Prof. Anantharam, Fall 2010</p>
</li>
</ul>
<h1>Hobbies</h1>
<p>I've previous spent a lot of time in a ceramics studio and helped found  <a href="http://ceramics.stanford.edu/">the Stanford ceramics studio</a>. These days, when I'm not working on my degree, I'm likely to be climbing or cooking, or <a href="http://usapl.liftingdatabase.com/lifters-view?id=30767">competing in powerlifting</a>.</p>
<div id="footer">
<div id="footer-text">
Page generated 2017-02-01 15:46:15 EST, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
